{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31a6b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:19.329932Z",
     "iopub.status.busy": "2024-07-12T13:26:19.329213Z",
     "iopub.status.idle": "2024-07-12T13:26:22.146858Z",
     "shell.execute_reply": "2024-07-12T13:26:22.146067Z"
    },
    "papermill": {
     "duration": 2.836724,
     "end_time": "2024-07-12T13:26:22.149219",
     "exception": false,
     "start_time": "2024-07-12T13:26:19.312495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import io\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "from pandas import read_csv\n",
    "from math import floor, ceil, sqrt, exp\n",
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import sys\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "parent_dir = os.path.abspath(os.path.join(CURRENT_DIR, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.helpers import crop_image\n",
    "from models.change_vit import Trainer, Encoder, Decoder, DinoVisionTransformer, PatchEmbed, Block, MemEffAttention, Mlp, BasicBlock, FeatureInjector, BlockInjector, CrossAttention, MlpDecoder, ResNet\n",
    "from models.efficientunet import CDUnet, UpSamplingBlock, ConvBlock\n",
    "from models.siamconc import SiamUnet_conc\n",
    "from models.siamdiff import SiamUnet_diff\n",
    "from models.stackunet import Unet\n",
    "from models.efficientunet_respath_attn import CDUnetResPath, Respath, BasicConv, GridAttentionBlock2D\n",
    "from models.model import SupervisedModel, SemiSupervisedModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae00de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"{CURRENT_DIR}/../datasets/concrete_cd_labeled-v2-2/concrete_cd_labeled-v2-2\"\n",
    "VALIDATION_PART = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf70225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:22.453760Z",
     "iopub.status.busy": "2024-07-12T13:26:22.453268Z",
     "iopub.status.idle": "2024-07-12T13:26:22.471399Z",
     "shell.execute_reply": "2024-07-12T13:26:22.470606Z"
    },
    "papermill": {
     "duration": 0.036641,
     "end_time": "2024-07-12T13:26:22.473303",
     "exception": false,
     "start_time": "2024-07-12T13:26:22.436662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChangeDetectionTestDataset(Dataset):\n",
    "    def __init__(self, data_path, transforms=None, mask_pattern=None):\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.test_regex = None if mask_pattern is None else re.compile(mask_pattern) \n",
    "        self.weights = [1.0, 1.0]\n",
    "        self.image_num = 0\n",
    "        \n",
    "        self.fetched_data = []\n",
    "        self._fetch_paths(data_path)\n",
    "\n",
    "    def _fetch_paths(self, data_path):\n",
    "        total_pixels = 0.0\n",
    "        positive_pixels = 0.0\n",
    "        \n",
    "        masks_path = os.path.join(data_path, \"masks\")\n",
    "        matched_test_files = os.listdir(masks_path) if self.test_regex is None else [f for f in os.listdir(masks_path) if self.test_regex.match(f)]\n",
    "        for mask_name in matched_test_files:\n",
    "            if len(mask_name) >= 4 and mask_name[-4:] == \".PNG\":\n",
    "                video_name, snapshot_name = mask_name.split(\"_\")\n",
    "                snapshot_name = snapshot_name.split('.')[0]\n",
    "                snapshots_dir_path = os.path.join(DATA_PATH, \"data\", video_name)\n",
    "                before_patches = crop_image(np.array(Image.open(f\"{snapshots_dir_path}/before_{snapshot_name}.png\")))\n",
    "                after_patches = crop_image(np.array(Image.open(f\"{snapshots_dir_path}/after_{snapshot_name}.png\")))\n",
    "                \n",
    "                uncropped_mask = np.array(Image.open(f\"{masks_path}/{mask_name}\"))[..., :1]\n",
    "                \n",
    "                total_pixels += np.prod(uncropped_mask.shape)\n",
    "                positive_pixels += uncropped_mask.sum()\n",
    "                mask_patches = crop_image(uncropped_mask)\n",
    "                \n",
    "                for before_sample, after_sample, mask_sample in zip(before_patches, after_patches, mask_patches):\n",
    "                    self.fetched_data.append({\"before\":before_sample, \"after\":after_sample, \"mask\":mask_sample})\n",
    "        self.image_num = len(matched_test_files)\n",
    "        \n",
    "    def augment_image_mask(\n",
    "        self,\n",
    "        patch_1: torch.Tensor,\n",
    "        patch_2: torch.Tensor,\n",
    "        mask: torch.Tensor, \n",
    "        probability: float = 0.8,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "       \n",
    "        if random.random() <= probability:\n",
    "            angle = random.randint(-50, 50)\n",
    "            patch_1 = TF.rotate(patch_1, angle)\n",
    "            patch_2 = TF.rotate(patch_2, angle)\n",
    "            mask = TF.rotate(mask, angle)\n",
    "        if random.random() <= probability:\n",
    "            patch_1 = TF.vflip(patch_1)\n",
    "            patch_2 = TF.vflip(patch_2)\n",
    "            mask = TF.vflip(mask)  \n",
    "        if random.random() <= probability:\n",
    "            patch_1 = TF.hflip(patch_1)\n",
    "            patch_2 = TF.hflip(patch_2)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        return patch_1, patch_2, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fetched_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_sample = self.fetched_data[idx]\n",
    "        before_patch = data_sample[\"before\"]\n",
    "        after_patch = data_sample[\"after\"]\n",
    "        mask = data_sample[\"mask\"]\n",
    "\n",
    "        before_patch, after_patch, mask = TF.to_tensor(before_patch), TF.to_tensor(after_patch), TF.to_tensor(mask)\n",
    "        before_patch, after_patch, mask = self.augment_image_mask(before_patch, after_patch, mask)\n",
    "        before_patch = TF.normalize(before_patch, mean=(0.485), std=(0.229))\n",
    "        after_patch = TF.normalize(after_patch, mean=(0.485), std=(0.229))\n",
    "\n",
    "        return {\"before\":before_patch, \"after\":after_patch, \"mask\":mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af752cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:22.505299Z",
     "iopub.status.busy": "2024-07-12T13:26:22.505046Z",
     "iopub.status.idle": "2024-07-12T13:26:40.997814Z",
     "shell.execute_reply": "2024-07-12T13:26:40.996770Z"
    },
    "papermill": {
     "duration": 18.51199,
     "end_time": "2024-07-12T13:26:41.000649",
     "exception": false,
     "start_time": "2024-07-12T13:26:22.488659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "dataset = ChangeDetectionTestDataset(DATA_PATH, mask_pattern=r\"^(?!4A_|5A_).*\")\n",
    "train_dataset, val_dataset, _ = random_split(dataset, [1 - VALIDATION_PART, VALIDATION_PART, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022d9d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:41.209658Z",
     "iopub.status.busy": "2024-07-12T13:26:41.209286Z",
     "iopub.status.idle": "2024-07-12T13:26:41.216368Z",
     "shell.execute_reply": "2024-07-12T13:26:41.215427Z"
    },
    "papermill": {
     "duration": 0.030339,
     "end_time": "2024-07-12T13:26:41.218560",
     "exception": false,
     "start_time": "2024-07-12T13:26:41.188221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        probs = probs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (probs * targets).sum()\n",
    "        dice_coeff = (2. * intersection + self.smooth) / (probs.sum() + targets.sum() + self.smooth)\n",
    "        dice_loss = 1 - dice_coeff\n",
    "        \n",
    "        return dice_loss\n",
    "\n",
    "loss_bce = nn.BCELoss()\n",
    "loss_dice = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_object(model_name):\n",
    "    if model_name == \"change_vit\": return Trainer(\"tiny\").float()\n",
    "    if model_name == \"siam_diff\": return SiamUnet_diff(3*1, 1)\n",
    "    if model_name == \"siam_conc\": return SiamUnet_conc(3*1, 1)\n",
    "    if model_name == \"stackunet\": return Unet(3*2, 1)\n",
    "    if model_name == \"stackunet\": return Unet(3*2, 1)\n",
    "    if model_name == \"efficientunet\": return CDUnet(out_channels=1, pretrained=True)\n",
    "    if model_name == \"efficientunet_respath_attn\":\n",
    "        output_model = CDUnetResPath(out_channels=1, pretrained=True)\n",
    "        output_model.activate_attention_gates()\n",
    "        return output_model\n",
    "    if model_name == \"efficientunet_respath\":\n",
    "        output_model = CDUnetResPath(out_channels=1, pretrained=True)\n",
    "        output_model.deactivate_attention_gates()\n",
    "        return output_model\n",
    "    return None\n",
    "\n",
    "models_training_configs = [\n",
    "    {\n",
    "        \"model_name\": \"efficientunet_respath\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [100, 100], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [512, 512], # [height, width]\n",
    "        \"lr\": [1e-2, 1e-3], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [16, 4],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x.freeze_backbone(),\n",
    "        \"unfreeze_function\": lambda x: x.unfreeze_backbone()\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"efficientunet_respath_attn\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [100, 100], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [512, 512], # [height, width]\n",
    "        \"lr\": [1e-2, 1e-3], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [16, 4],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x.freeze_backbone(),\n",
    "        \"unfreeze_function\": lambda x: x.unfreeze_backbone()\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"efficientunet\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [100, 100], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [512, 512], # [height, width]\n",
    "        \"lr\": [1e-2, 1e-3], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [16, 4],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x.freeze_backbone(),\n",
    "        \"unfreeze_function\": lambda x: x.unfreeze_backbone()\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"change_vit\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [0, 200], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [256, 256], # [height, width]\n",
    "        \"lr\": [0, 1e-3], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [0, 8],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x,\n",
    "        \"unfreeze_function\": lambda x: x\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"siam_diff\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [0, 200], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [500, 500], # [height, width]\n",
    "        \"lr\": [0, 1e-2], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [0, 16],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x,\n",
    "        \"unfreeze_function\": lambda x: x\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"siam_conc\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [0, 200], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [500, 500], # [height, width]\n",
    "        \"lr\": [0, 1e-2], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [0, 16],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x,\n",
    "        \"unfreeze_function\": lambda x: x\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"stackunet\",\n",
    "        \"save_dir\": f\"{CURRENT_DIR}/../weights/change_detection\",\n",
    "        \"epochs\": [0, 200], # [epochs_freeze, epochs_unfreeze]\n",
    "        \"input_size\": [500, 500], # [height, width]\n",
    "        \"lr\": [0, 1e-2], # [lr_freeze, lr_unfreeze]\n",
    "        \"batch_size\": [0, 16],  # [batch_size_freeze, batch_size_unfreeze]\n",
    "        \"eval_epoch\": 5, # evaluate  the model every \"eval_epoch\" epochs,\n",
    "        \"device\": device,\n",
    "        \"freeze_function\": lambda x: x,\n",
    "        \"unfreeze_function\": lambda x: x\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd63f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eval_loss(model, val_loader, resizer, device):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            befores = resizer(batch['before']).float().to(device)\n",
    "            afters = resizer(batch['after']).float().to(device)\n",
    "            masks = resizer(batch['mask']).float().to(device)\n",
    "            masks = torch.clamp(masks, 0.0, 1.0)\n",
    "\n",
    "            output = model(befores, afters)\n",
    "            loss = loss_bce(output, masks) + loss_dice(output, masks)\n",
    "            eval_loss += loss.item()\n",
    "    model.train()\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, optimizer, model_training_configs, stage_name=\"\"):\n",
    "    eval_epoch = model_training_configs[\"eval_epoch\"]\n",
    "    best_eval_loss = None\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
    "    resizer = torchvision.transforms.Resize(model_training_configs[\"input_size\"])\n",
    "    for epoch_index in range(epochs):\n",
    "        print(f'[{stage_name}] Epoch: ' + str(epoch_index + 1) + ' of ' + str(epochs))\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            befores = resizer(batch['before']).float().to(model_training_configs[\"device\"])\n",
    "            afters = resizer(batch['after']).float().to(model_training_configs[\"device\"])\n",
    "            masks = resizer(batch['mask']).float().to(model_training_configs[\"device\"])\n",
    "            masks = torch.clamp(masks, 0.0, 1.0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(befores, afters)\n",
    "            loss = loss_bce(output, masks)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch_index + 1) % eval_epoch == 0:\n",
    "            current_eval_loss = compute_eval_loss(model, val_loader, resizer, model_training_configs[\"device\"])\n",
    "            if best_eval_loss is None or best_eval_loss > current_eval_loss:\n",
    "                best_eval_loss = current_eval_loss\n",
    "                torch.save(model, f'{model_training_configs[\"save_dir\"]}/{model_training_configs[\"model_name\"]}_{stage_name}_loss_{int(current_eval_loss)}.pth')\n",
    "            print(f\"Eval Loss: {current_eval_loss}\")\n",
    "\n",
    "        print(epoch_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "    torch.save(model, f'{model_training_configs[\"save_dir\"]}/{model_training_configs[\"model_name\"]}_{stage_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8829bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:42.951475Z",
     "iopub.status.busy": "2024-07-12T13:26:42.951003Z",
     "iopub.status.idle": "2024-07-12T16:01:12.499321Z",
     "shell.execute_reply": "2024-07-12T16:01:12.498369Z"
    },
    "papermill": {
     "duration": 9269.60773,
     "end_time": "2024-07-12T16:01:12.537559",
     "exception": false,
     "start_time": "2024-07-12T13:26:42.929829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[freeze] Epoch: 1 of 100\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for model_training_configs in models_training_configs:\n",
    "    model = create_model_object(model_training_configs[\"model_name\"]).to(model_training_configs[\"device\"])\n",
    "    model.train()\n",
    "\n",
    "    freeze_epochs = model_training_configs[\"epochs\"][0]\n",
    "    if freeze_epochs != 0:\n",
    "        model_training_configs[\"freeze_function\"](model)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=model_training_configs[\"batch_size\"][0], shuffle=True)\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), lr=model_training_configs[\"lr\"][0], weight_decay=1e-4)\n",
    "        train(model, train_loader, val_loader, freeze_epochs, optimizer, model_training_configs, \"freeze\")\n",
    "        model_training_configs[\"unfreeze_function\"](model)\n",
    "\n",
    "    unfreeze_epochs = model_training_configs[\"epochs\"][1]\n",
    "    if unfreeze_epochs != 0:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=model_training_configs[\"batch_size\"][1], shuffle=True)\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), lr=model_training_configs[\"lr\"][1], weight_decay=1e-4)\n",
    "        train(model, train_loader, val_loader, unfreeze_epochs, optimizer, model_training_configs, \"unfreeze\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5301964,
     "sourceId": 8813995,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5174086,
     "sourceId": 8849544,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 185975202,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9334.358104,
   "end_time": "2024-07-12T16:01:15.500733",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-12T13:25:41.142629",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
